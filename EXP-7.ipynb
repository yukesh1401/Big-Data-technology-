{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNI5hp6zD4I8vC6M7tsYO/t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"u5LGq0LCpVaU","executionInfo":{"status":"ok","timestamp":1760343944169,"user_tz":-330,"elapsed":87698,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}}},"outputs":[],"source":["# ============================================================\n","# ðŸ§© Step 1: Install Hadoop (minimal clean setup)\n","# ============================================================\n","\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\n","!tar -xzf hadoop-3.4.1.tar.gz\n","!mv hadoop-3.4.1 /usr/local/hadoop\n","\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n","os.environ[\"PATH\"] += \":/usr/local/hadoop/bin:/usr/local/hadoop/sbin\"\n","\n","# Configure Hadoop environment\n","!echo \"export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\" >> /usr/local/hadoop/etc/hadoop/hadoop-env.sh\n"]},{"cell_type":"code","source":["# ============================================================\n","# ðŸ“¦ Step 2: Prepare Sample Weather Data\n","# ============================================================\n","\n","!mkdir -p /content/weather_data\n","\n","sample_data = \"\"\"19010101,34\n","19010102,38\n","19010103,29\n","19010104,31\n","19020101,30\n","19020102,33\n","19020103,28\n","19020104,36\n","19030101,40\n","19030102,41\n","19030103,35\n","19030104,39\n","\"\"\"\n","\n","with open(\"/content/weather_data/temperature.txt\", \"w\") as f:\n","    f.write(sample_data)\n","\n","!cat /content/weather_data/temperature.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yA9obNQiquQl","executionInfo":{"status":"ok","timestamp":1760344053944,"user_tz":-330,"elapsed":214,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"df129d43-4fbf-44c1-90a3-0b9eda1a7cd7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["19010101,34\n","19010102,38\n","19010103,29\n","19010104,31\n","19020101,30\n","19020102,33\n","19020103,28\n","19020104,36\n","19030101,40\n","19030102,41\n","19030103,35\n","19030104,39\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hjISngWaqwMr","executionInfo":{"status":"ok","timestamp":1760343944428,"user_tz":-330,"elapsed":50,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"a82d7768-21d0-4938-ddea-1d1a4a1aa66a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/mapper.py\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# ðŸš€ Step 4: Run MapReduce Using Hadoop Streaming (fixed)\n","# ============================================================\n","\n","# Remove existing output directory (Hadoop can't overwrite)\n","!hdfs dfs -rm -r -f /content/weather_output || rm -rf /content/weather_output\n","\n","# Run the MapReduce job\n","!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n","    -input /content/weather_data/temperature.txt \\\n","    -output /content/weather_output \\\n","    -mapper \"python3 /content/mapper.py\" \\\n","    -reducer \"python3 /content/reducer.py\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DIVmnH2rq2gG","executionInfo":{"status":"ok","timestamp":1760344123692,"user_tz":-330,"elapsed":5357,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"1be64d78-e4eb-40fc-9037-cc2aaeb03556"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-10-13 08:28:39,668 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","Deleted /content/weather_output\n","2025-10-13 08:28:41,521 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2025-10-13 08:28:41,671 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2025-10-13 08:28:41,671 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2025-10-13 08:28:41,693 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2025-10-13 08:28:41,939 INFO mapred.FileInputFormat: Total input files to process : 1\n","2025-10-13 08:28:41,964 INFO mapreduce.JobSubmitter: number of splits:1\n","2025-10-13 08:28:42,215 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1790326985_0001\n","2025-10-13 08:28:42,215 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2025-10-13 08:28:42,423 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2025-10-13 08:28:42,424 INFO mapreduce.Job: Running job: job_local1790326985_0001\n","2025-10-13 08:28:42,434 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2025-10-13 08:28:42,444 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2025-10-13 08:28:42,450 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2025-10-13 08:28:42,450 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2025-10-13 08:28:42,508 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2025-10-13 08:28:42,510 INFO mapred.LocalJobRunner: Starting task: attempt_local1790326985_0001_m_000000_0\n","2025-10-13 08:28:42,546 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2025-10-13 08:28:42,548 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2025-10-13 08:28:42,567 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2025-10-13 08:28:42,575 INFO mapred.MapTask: Processing split: file:/content/weather_data/temperature.txt:0+144\n","2025-10-13 08:28:42,591 INFO mapred.MapTask: numReduceTasks: 1\n","2025-10-13 08:28:42,706 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2025-10-13 08:28:42,706 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2025-10-13 08:28:42,706 INFO mapred.MapTask: soft limit at 83886080\n","2025-10-13 08:28:42,706 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2025-10-13 08:28:42,706 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2025-10-13 08:28:42,714 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2025-10-13 08:28:42,728 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/python3, /content/mapper.py]\n","2025-10-13 08:28:42,741 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2025-10-13 08:28:42,746 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2025-10-13 08:28:42,746 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2025-10-13 08:28:42,747 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2025-10-13 08:28:42,751 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2025-10-13 08:28:42,752 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2025-10-13 08:28:42,755 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2025-10-13 08:28:42,756 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2025-10-13 08:28:42,756 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2025-10-13 08:28:42,757 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2025-10-13 08:28:42,761 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2025-10-13 08:28:42,762 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2025-10-13 08:28:42,796 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2025-10-13 08:28:42,796 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2025-10-13 08:28:42,879 INFO streaming.PipeMapRed: Records R/W=12/1\n","2025-10-13 08:28:42,894 INFO streaming.PipeMapRed: MRErrorThread done\n","2025-10-13 08:28:42,895 INFO streaming.PipeMapRed: mapRedFinished\n","2025-10-13 08:28:42,900 INFO mapred.LocalJobRunner: \n","2025-10-13 08:28:42,902 INFO mapred.MapTask: Starting flush of map output\n","2025-10-13 08:28:42,902 INFO mapred.MapTask: Spilling map output\n","2025-10-13 08:28:42,902 INFO mapred.MapTask: bufstart = 0; bufend = 96; bufvoid = 104857600\n","2025-10-13 08:28:42,902 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214352(104857408); length = 45/6553600\n","2025-10-13 08:28:42,914 INFO mapred.MapTask: Finished spill 0\n","2025-10-13 08:28:42,940 INFO mapred.Task: Task:attempt_local1790326985_0001_m_000000_0 is done. And is in the process of committing\n","2025-10-13 08:28:42,946 INFO mapred.LocalJobRunner: Records R/W=12/1\n","2025-10-13 08:28:42,946 INFO mapred.Task: Task 'attempt_local1790326985_0001_m_000000_0' done.\n","2025-10-13 08:28:42,955 INFO mapred.Task: Final Counters for attempt_local1790326985_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=142070\n","\t\tFILE: Number of bytes written=860348\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=12\n","\t\tMap output records=12\n","\t\tMap output bytes=96\n","\t\tMap output materialized bytes=126\n","\t\tInput split bytes=94\n","\t\tCombine input records=0\n","\t\tSpilled Records=12\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=347078656\n","\tFile Input Format Counters \n","\t\tBytes Read=144\n","2025-10-13 08:28:42,956 INFO mapred.LocalJobRunner: Finishing task: attempt_local1790326985_0001_m_000000_0\n","2025-10-13 08:28:42,957 INFO mapred.LocalJobRunner: map task executor complete.\n","2025-10-13 08:28:42,964 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2025-10-13 08:28:42,964 INFO mapred.LocalJobRunner: Starting task: attempt_local1790326985_0001_r_000000_0\n","2025-10-13 08:28:42,976 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2025-10-13 08:28:42,976 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2025-10-13 08:28:42,977 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2025-10-13 08:28:42,979 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@666c6b29\n","2025-10-13 08:28:42,981 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2025-10-13 08:28:43,016 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2025-10-13 08:28:43,025 INFO reduce.EventFetcher: attempt_local1790326985_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2025-10-13 08:28:43,083 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1790326985_0001_m_000000_0 decomp: 122 len: 126 to MEMORY\n","2025-10-13 08:28:43,090 INFO reduce.InMemoryMapOutput: Read 122 bytes from map-output for attempt_local1790326985_0001_m_000000_0\n","2025-10-13 08:28:43,093 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 122, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->122\n","2025-10-13 08:28:43,098 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2025-10-13 08:28:43,099 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2025-10-13 08:28:43,100 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2025-10-13 08:28:43,111 INFO mapred.Merger: Merging 1 sorted segments\n","2025-10-13 08:28:43,111 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 115 bytes\n","2025-10-13 08:28:43,114 INFO reduce.MergeManagerImpl: Merged 1 segments, 122 bytes to disk to satisfy reduce memory limit\n","2025-10-13 08:28:43,115 INFO reduce.MergeManagerImpl: Merging 1 files, 126 bytes from disk\n","2025-10-13 08:28:43,115 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2025-10-13 08:28:43,116 INFO mapred.Merger: Merging 1 sorted segments\n","2025-10-13 08:28:43,116 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 115 bytes\n","2025-10-13 08:28:43,119 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2025-10-13 08:28:43,131 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/python3, /content/reducer.py]\n","2025-10-13 08:28:43,138 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2025-10-13 08:28:43,142 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2025-10-13 08:28:43,164 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2025-10-13 08:28:43,165 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2025-10-13 08:28:43,285 INFO streaming.PipeMapRed: Records R/W=12/1\n","2025-10-13 08:28:43,293 INFO streaming.PipeMapRed: MRErrorThread done\n","2025-10-13 08:28:43,294 INFO streaming.PipeMapRed: mapRedFinished\n","2025-10-13 08:28:43,295 INFO mapred.Task: Task:attempt_local1790326985_0001_r_000000_0 is done. And is in the process of committing\n","2025-10-13 08:28:43,297 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2025-10-13 08:28:43,297 INFO mapred.Task: Task attempt_local1790326985_0001_r_000000_0 is allowed to commit now\n","2025-10-13 08:28:43,327 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1790326985_0001_r_000000_0' to file:/content/weather_output\n","2025-10-13 08:28:43,333 INFO mapred.LocalJobRunner: Records R/W=12/1 > reduce\n","2025-10-13 08:28:43,334 INFO mapred.Task: Task 'attempt_local1790326985_0001_r_000000_0' done.\n","2025-10-13 08:28:43,335 INFO mapred.Task: Final Counters for attempt_local1790326985_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=142354\n","\t\tFILE: Number of bytes written=860543\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=3\n","\t\tReduce shuffle bytes=126\n","\t\tReduce input records=12\n","\t\tReduce output records=3\n","\t\tSpilled Records=12\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=25\n","\t\tTotal committed heap usage (bytes)=347078656\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=69\n","2025-10-13 08:28:43,335 INFO mapred.LocalJobRunner: Finishing task: attempt_local1790326985_0001_r_000000_0\n","2025-10-13 08:28:43,335 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2025-10-13 08:28:43,442 INFO mapreduce.Job: Job job_local1790326985_0001 running in uber mode : false\n","2025-10-13 08:28:43,443 INFO mapreduce.Job:  map 100% reduce 100%\n","2025-10-13 08:28:43,448 INFO mapreduce.Job: Job job_local1790326985_0001 completed successfully\n","2025-10-13 08:28:43,466 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=284424\n","\t\tFILE: Number of bytes written=1720891\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=12\n","\t\tMap output records=12\n","\t\tMap output bytes=96\n","\t\tMap output materialized bytes=126\n","\t\tInput split bytes=94\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=3\n","\t\tReduce shuffle bytes=126\n","\t\tReduce input records=12\n","\t\tReduce output records=3\n","\t\tSpilled Records=24\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=25\n","\t\tTotal committed heap usage (bytes)=694157312\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=144\n","\tFile Output Format Counters \n","\t\tBytes Written=69\n","2025-10-13 08:28:43,466 INFO streaming.StreamJob: Output directory: /content/weather_output\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# ðŸ“Š Step 5: View the Output\n","# ============================================================\n","\n","!cat /content/weather_output/part-00000\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZ0uEXXKrEfO","executionInfo":{"status":"ok","timestamp":1760344135626,"user_tz":-330,"elapsed":121,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"c991faf7-de76-47e6-bfe6-779f8210e7f8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["1901\tMin=29\tMax=38\n","1902\tMin=28\tMax=36\n","1903\tMin=35\tMax=41\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# ðŸ§© Step 1: Install Java and Apache Pig\n","# ============================================================\n","\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q https://downloads.apache.org/pig/pig-0.17.0/pig-0.17.0.tar.gz\n","!tar -xzf pig-0.17.0.tar.gz\n","!mv pig-0.17.0 /usr/local/pig\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"PIG_HOME\"] = \"/usr/local/pig\"\n","os.environ[\"PATH\"] += \":/usr/local/pig/bin\"\n"],"metadata":{"id":"o7KsuuvPrSEX","executionInfo":{"status":"ok","timestamp":1760344456348,"user_tz":-330,"elapsed":19276,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# ðŸ“„ Step 2: Create Sample Employee Data\n","# ============================================================\n","\n","data = \"\"\"1,Arun,IT,35000\n","2,Bala,HR,28000\n","3,Kiran,IT,42000\n","4,Sara,Finance,39000\n","5,Devi,HR,31000\n","6,Ram,Finance,45000\n","\"\"\"\n","\n","with open(\"/content/employee.txt\", \"w\") as f:\n","    f.write(data)\n","\n","!cat /content/employee.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8UnsDDyVs880","executionInfo":{"status":"ok","timestamp":1760344620604,"user_tz":-330,"elapsed":125,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"8cc4b2e8-1d4c-4064-ab6d-9d5538ac1c82"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["1,Arun,IT,35000\n","2,Bala,HR,28000\n","3,Kiran,IT,42000\n","4,Sara,Finance,39000\n","5,Devi,HR,31000\n","6,Ram,Finance,45000\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# ðŸš€ Step 4: Run Pig Script Locally\n","# ============================================================\n","\n","!pig -x local /content/experiment5.pig\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SSHaexsWtKvb","executionInfo":{"status":"ok","timestamp":1760344741250,"user_tz":-330,"elapsed":6770,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"16daf884-d2ba-490f-c759-8f01041ee0ff"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-10-13 08:38:56,348 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL\n","2025-10-13 08:38:56,349 INFO pig.ExecTypeProvider: Picked LOCAL as the ExecType\n","2025-10-13 08:38:56,439 [main] INFO  org.apache.pig.Main - Apache Pig version 0.17.0 (r1797386) compiled Jun 02 2017, 15:41:58\n","2025-10-13 08:38:56,439 [main] INFO  org.apache.pig.Main - Logging error messages to: /content/pig_1760344736436.log\n","2025-10-13 08:38:56,458 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - user.name is deprecated. Instead, use mapreduce.job.user.name\n","2025-10-13 08:38:56,672 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\n","2025-10-13 08:38:56,753 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2025-10-13 08:38:56,755 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///\n","2025-10-13 08:38:56,787 [main] INFO  org.apache.pig.PigServer - Pig Script ID for the session: PIG-experiment5.pig-96696e0f-58ad-4bba-864a-06ffa13ab48c\n","2025-10-13 08:38:56,787 [main] WARN  org.apache.pig.PigServer - ATS is disabled since yarn.timeline-service.enabled set to false\n","2025-10-13 08:38:57,571 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n","2025-10-13 08:38:57,604 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: GROUP_BY\n","2025-10-13 08:38:57,815 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NestedLimitOptimizer, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}\n","2025-10-13 08:38:57,954 [main] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n","2025-10-13 08:38:58,050 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\n","2025-10-13 08:38:58,072 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.CombinerOptimizerUtil - Choosing to move algebraic foreach to combiner\n","2025-10-13 08:38:58,123 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\n","2025-10-13 08:38:58,124 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\n","2025-10-13 08:38:58,351 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsConfig - Loaded properties from hadoop-metrics2.properties\n","2025-10-13 08:38:58,557 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).\n","2025-10-13 08:38:58,558 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started\n","2025-10-13 08:38:58,584 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.MRScriptState - Pig script settings are added to the job\n","2025-10-13 08:38:58,591 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n","2025-10-13 08:38:58,592 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n","2025-10-13 08:38:58,593 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n","2025-10-13 08:38:58,595 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # of required reducers.\n","2025-10-13 08:38:58,596 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator\n","2025-10-13 08:38:58,630 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=106\n","2025-10-13 08:38:58,632 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n","2025-10-13 08:38:58,632 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n","2025-10-13 08:38:58,658 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n","2025-10-13 08:38:58,675 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n","2025-10-13 08:38:58,675 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cacche\n","2025-10-13 08:38:58,675 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /tmp/1760344738675-0\n","2025-10-13 08:38:58,867 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n","2025-10-13 08:38:58,895 [JobControl] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n","2025-10-13 08:38:58,934 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2025-10-13 08:38:59,118 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n","2025-10-13 08:38:59,145 [JobControl] INFO  org.apache.pig.builtin.PigStorage - Using PigTextInputFormat\n","2025-10-13 08:38:59,155 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n","2025-10-13 08:38:59,155 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n","2025-10-13 08:38:59,182 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n","2025-10-13 08:38:59,309 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n","2025-10-13 08:38:59,791 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1524945196_0001\n","2025-10-13 08:38:59,791 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []\n","2025-10-13 08:39:00,051 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n","2025-10-13 08:39:00,052 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local1524945196_0001\n","2025-10-13 08:39:00,052 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases avg_salary,emp,grouped\n","2025-10-13 08:39:00,052 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: emp[2,6],emp[-1,-1],avg_salary[18,13],grouped[15,10] C: avg_salary[18,13],grouped[15,10] R: avg_salary[18,13]\n","2025-10-13 08:39:00,057 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n","2025-10-13 08:39:00,083 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n","2025-10-13 08:39:00,083 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_local1524945196_0001]\n","2025-10-13 08:39:00,116 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n","2025-10-13 08:39:00,118 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n","2025-10-13 08:39:00,118 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2025-10-13 08:39:00,118 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n","2025-10-13 08:39:00,121 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n","2025-10-13 08:39:00,122 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n","2025-10-13 08:39:00,122 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2025-10-13 08:39:00,123 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n","2025-10-13 08:39:00,185 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n","2025-10-13 08:39:00,185 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1524945196_0001_m_000000_0\n","2025-10-13 08:39:00,241 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n","2025-10-13 08:39:00,242 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n","2025-10-13 08:39:00,242 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2025-10-13 08:39:00,263 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n","2025-10-13 08:39:00,274 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n","Total Length = 106\n","Input split[0]:\n","   Length = 106\n","   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n","   Locations:\n","\n","-----------------------\n","\n","2025-10-13 08:39:00,287 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.builtin.PigStorage - Using PigTextInputFormat\n","2025-10-13 08:39:00,291 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/employee.txt:0+106\n","2025-10-13 08:39:00,363 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n","2025-10-13 08:39:00,363 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n","2025-10-13 08:39:00,363 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n","2025-10-13 08:39:00,363 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n","2025-10-13 08:39:00,363 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n","2025-10-13 08:39:00,368 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2025-10-13 08:39:00,380 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n","2025-10-13 08:39:00,382 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.\n","2025-10-13 08:39:00,398 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: emp[2,6],emp[-1,-1],avg_salary[18,13],grouped[15,10] C: avg_salary[18,13],grouped[15,10] R: avg_salary[18,13]\n","2025-10-13 08:39:00,409 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - \n","2025-10-13 08:39:00,410 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n","2025-10-13 08:39:00,410 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n","2025-10-13 08:39:00,410 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 96; bufvoid = 104857600\n","2025-10-13 08:39:00,410 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 26214376(104857504); length = 21/6553600\n","2025-10-13 08:39:00,436 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine - Aliases being processed per job phase (AliasName[line,offset]): M: emp[2,6],emp[-1,-1],avg_salary[18,13],grouped[15,10] C: avg_salary[18,13],grouped[15,10] R: avg_salary[18,13]\n","2025-10-13 08:39:00,446 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n","2025-10-13 08:39:00,459 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1524945196_0001_m_000000_0 is done. And is in the process of committing\n","2025-10-13 08:39:00,460 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map\n","2025-10-13 08:39:00,461 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1524945196_0001_m_000000_0' done.\n","2025-10-13 08:39:00,469 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1524945196_0001_m_000000_0: Counters: 18\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=513\n","\t\tFILE: Number of bytes written=694975\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=6\n","\t\tMap output records=6\n","\t\tMap output bytes=96\n","\t\tMap output materialized bytes=65\n","\t\tInput split bytes=353\n","\t\tCombine input records=6\n","\t\tCombine output records=3\n","\t\tSpilled Records=3\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=336592896\n","\tFile Input Format Counters \n","\t\tBytes Read=0\n","2025-10-13 08:39:00,469 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1524945196_0001_m_000000_0\n","2025-10-13 08:39:00,470 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n","2025-10-13 08:39:00,474 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1524945196_0001_r_000000_0\n","2025-10-13 08:39:00,474 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks\n","2025-10-13 08:39:00,500 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n","2025-10-13 08:39:00,500 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n","2025-10-13 08:39:00,501 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2025-10-13 08:39:00,508 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n","2025-10-13 08:39:00,514 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@27455aad\n","2025-10-13 08:39:00,518 [pool-4-thread-1] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n","2025-10-13 08:39:00,540 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=734003200, maxSingleShuffleLimit=183500800, mergeThreshold=484442144, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2025-10-13 08:39:00,543 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local1524945196_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2025-10-13 08:39:00,583 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete\n","2025-10-13 08:39:00,583 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_local1524945196_0001]\n","2025-10-13 08:39:00,591 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local1524945196_0001_m_000000_0 decomp: 61 len: 65 to MEMORY\n","2025-10-13 08:39:00,595 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 61 bytes from map-output for attempt_local1524945196_0001_m_000000_0\n","2025-10-13 08:39:00,597 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 61, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->61\n","2025-10-13 08:39:00,601 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning\n","2025-10-13 08:39:00,602 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n","2025-10-13 08:39:00,602 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2025-10-13 08:39:00,609 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n","2025-10-13 08:39:00,609 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 54 bytes\n","2025-10-13 08:39:00,612 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 1 segments, 61 bytes to disk to satisfy reduce memory limit\n","2025-10-13 08:39:00,612 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 65 bytes from disk\n","2025-10-13 08:39:00,613 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce\n","2025-10-13 08:39:00,613 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n","2025-10-13 08:39:00,614 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 54 bytes\n","2025-10-13 08:39:00,615 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n","2025-10-13 08:39:00,623 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n","2025-10-13 08:39:00,624 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n","2025-10-13 08:39:00,624 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2025-10-13 08:39:00,628 [pool-4-thread-1] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2025-10-13 08:39:00,630 [pool-4-thread-1] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n","2025-10-13 08:39:00,631 [pool-4-thread-1] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n","2025-10-13 08:39:00,640 [pool-4-thread-1] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce - Aliases being processed per job phase (AliasName[line,offset]): M: emp[2,6],emp[-1,-1],avg_salary[18,13],grouped[15,10] C: avg_salary[18,13],grouped[15,10] R: avg_salary[18,13]\n","2025-10-13 08:39:00,647 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1524945196_0001_r_000000_0 is done. And is in the process of committing\n","2025-10-13 08:39:00,656 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n","2025-10-13 08:39:00,657 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local1524945196_0001_r_000000_0 is allowed to commit now\n","2025-10-13 08:39:00,699 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local1524945196_0001_r_000000_0' to file:/content/output_pig\n","2025-10-13 08:39:00,700 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n","2025-10-13 08:39:00,700 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1524945196_0001_r_000000_0' done.\n","2025-10-13 08:39:00,701 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1524945196_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=675\n","\t\tFILE: Number of bytes written=695090\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=3\n","\t\tReduce shuffle bytes=65\n","\t\tReduce input records=3\n","\t\tReduce output records=3\n","\t\tSpilled Records=3\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=30\n","\t\tTotal committed heap usage (bytes)=336592896\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=0\n","2025-10-13 08:39:00,701 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1524945196_0001_r_000000_0\n","2025-10-13 08:39:00,701 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.\n","2025-10-13 08:39:00,866 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n","2025-10-13 08:39:00,879 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n","2025-10-13 08:39:00,880 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2025-10-13 08:39:00,882 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n","2025-10-13 08:39:00,918 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n","2025-10-13 08:39:00,921 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.SimplePigStats - Script Statistics: \n","\n","HadoopVersion\tPigVersion\tUserId\tStartedAt\tFinishedAt\tFeatures\n","3.4.1\t0.17.0\troot\t2025-10-13 08:38:58\t2025-10-13 08:39:00\tGROUP_BY\n","\n","Success!\n","\n","Job Stats (time in seconds):\n","JobId\tMaps\tReduces\tMaxMapTime\tMinMapTime\tAvgMapTime\tMedianMapTime\tMaxReduceTime\tMinReduceTime\tAvgReduceTime\tMedianReducetime\tAlias\tFeature\tOutputs\n","job_local1524945196_0001\t1\t1\tn/a\tn/a\tn/a\tn/a\tn/a\tn/a\tn/a\tn/a\tavg_salary,emp,grouped\tGROUP_BY,COMBINER\t/content/output_pig,\n","\n","Input(s):\n","Successfully read 6 records from: \"/content/employee.txt\"\n","\n","Output(s):\n","Successfully stored 3 records in: \"/content/output_pig\"\n","\n","Counters:\n","Total records written : 3\n","Total bytes written : 0\n","Spillable Memory Manager spill count : 0\n","Total bags proactively spilled: 0\n","Total records proactively spilled: 0\n","\n","Job DAG:\n","job_local1524945196_0001\n","\n","\n","2025-10-13 08:39:00,926 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n","2025-10-13 08:39:00,929 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n","2025-10-13 08:39:00,931 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n","2025-10-13 08:39:00,940 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n","2025-10-13 08:39:00,967 [main] INFO  org.apache.pig.Main - Pig script completed in 4 seconds and 901 milliseconds (4901 ms)\n"]}]},{"cell_type":"code","source":["!cat /content/output_pig/part-r-00000\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXcvHvBztU3y","executionInfo":{"status":"ok","timestamp":1760344638086,"user_tz":-330,"elapsed":113,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"bd925ca7-1795-4397-d254-8f9df7ed457b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["cat: /content/output_pig/part-r-00000: No such file or directory\n"]}]},{"cell_type":"code","source":["# Run the script\n","!pig -x local /content/experiment5.pig\n","\n","# View output\n","!cat /content/output_pig/part-r-00000\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oI46ukGCtv-I","executionInfo":{"status":"ok","timestamp":1760344765376,"user_tz":-330,"elapsed":4387,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"4279712f-6725-4e9c-8318-af69b2f63f64"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-10-13 08:39:23,850 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL\n","2025-10-13 08:39:23,850 INFO pig.ExecTypeProvider: Picked LOCAL as the ExecType\n","2025-10-13 08:39:23,933 [main] INFO  org.apache.pig.Main - Apache Pig version 0.17.0 (r1797386) compiled Jun 02 2017, 15:41:58\n","2025-10-13 08:39:23,933 [main] INFO  org.apache.pig.Main - Logging error messages to: /content/pig_1760344763930.log\n","2025-10-13 08:39:23,953 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - user.name is deprecated. Instead, use mapreduce.job.user.name\n","2025-10-13 08:39:24,172 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\n","2025-10-13 08:39:24,273 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2025-10-13 08:39:24,275 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///\n","2025-10-13 08:39:24,305 [main] INFO  org.apache.pig.PigServer - Pig Script ID for the session: PIG-experiment5.pig-3989e8cc-ae90-4aa3-bb8d-e60b7055c0ad\n","2025-10-13 08:39:24,305 [main] WARN  org.apache.pig.PigServer - ATS is disabled since yarn.timeline-service.enabled set to false\n","2025-10-13 08:39:24,890 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n","2025-10-13 08:39:24,894 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 6000: <file /content/experiment5.pig, line 21, column 0> Output Location Validation Failed for: '/content/output_pig More info to follow:\n","Output directory file:/content/output_pig already exists\n","Details at logfile: /content/pig_1760344763930.log\n","2025-10-13 08:39:24,934 [main] INFO  org.apache.pig.Main - Pig script completed in 1 second and 481 milliseconds (1481 ms)\n","HR,29500.0\n","IT,38500.0\n","Finance,42000.0\n"]}]},{"cell_type":"code","source":["# Download Hive 3.1.4\n","!wget https://downloads.apache.org/hive/hive-3.1.4/apache-hive-3.1.4-bin.tar.gz\n","!tar -xzf apache-hive-3.1.4-bin.tar.gz\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8AwT11nuWGr","executionInfo":{"status":"ok","timestamp":1760345256737,"user_tz":-330,"elapsed":1016,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"df95b69b-1d4f-4953-cc80-f13ca1546f01"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-10-13 08:47:35--  https://downloads.apache.org/hive/hive-3.1.4/apache-hive-3.1.4-bin.tar.gz\n","Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f9:3a:2c57::2, ...\n","Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n","HTTP request sent, awaiting response... 404 Not Found\n","2025-10-13 08:47:36 ERROR 404: Not Found.\n","\n","tar (child): apache-hive-3.1.4-bin.tar.gz: Cannot open: No such file or directory\n","tar (child): Error is not recoverable: exiting now\n","tar: Child returned status 2\n","tar: Error is not recoverable: exiting now\n"]}]},{"cell_type":"code","source":["import os\n","\n","os.environ[\"HIVE_HOME\"] = \"/content/apache-hive-3.1.4-bin\"\n","os.environ[\"PATH\"] = os.environ[\"HIVE_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n"],"metadata":{"id":"jSEsy79jwRqj","executionInfo":{"status":"ok","timestamp":1760345327930,"user_tz":-330,"elapsed":15,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["!schematool -dbType derby -initSchema\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KldRnWT6wUFI","executionInfo":{"status":"ok","timestamp":1760345280862,"user_tz":-330,"elapsed":106,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"61237654-12ad-46bf-9848-fcb0555ec505"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: schematool: command not found\n"]}]},{"cell_type":"code","source":["!ls $HIVE_HOME/bin/schematool\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxO9y1iiwiTz","executionInfo":{"status":"ok","timestamp":1760345337268,"user_tz":-330,"elapsed":112,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"5562cf51-039b-4c9d-f199-50b83039d07a"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access '/content/apache-hive-3.1.4-bin/bin/schematool': No such file or directory\n"]}]},{"cell_type":"code","source":["!hadoop fs -mkdir -p /user/hive/warehouse\n","!hadoop fs -mkdir /content/hive_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ah1-jDTkuogA","executionInfo":{"status":"ok","timestamp":1760345148995,"user_tz":-330,"elapsed":2625,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"2e9d3560-cb4e-494f-f162-d6f38926b0fe"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: `/content/hive_data': File exists\n"]}]},{"cell_type":"code","source":["import sqlite3\n","import pandas as pd\n","\n","# Create a sample database\n","conn = sqlite3.connect('/content/company.db')\n","cursor = conn.cursor()\n","\n","# Create employee table\n","cursor.execute('''\n","CREATE TABLE IF NOT EXISTS employee (\n","    id INT,\n","    name TEXT,\n","    dept TEXT,\n","    salary INT\n",")\n","''')\n","\n","# Insert sample data\n","cursor.executemany('''\n","INSERT INTO employee (id, name, dept, salary) VALUES (?, ?, ?, ?)\n","''', [\n","    (1, 'Alice', 'HR', 35000),\n","    (2, 'Bob', 'IT', 40000),\n","    (3, 'Charlie', 'Finance', 38000)\n","])\n","\n","conn.commit()\n"],"metadata":{"id":"1C8v8QkVur-Y","executionInfo":{"status":"ok","timestamp":1760345513920,"user_tz":-330,"elapsed":14,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["!pip install pyspark\n","\n","from pyspark.sql import SparkSession\n","import pandas as pd\n","\n","# Initialize Spark\n","spark = SparkSession.builder.appName(\"SqoopSim\").getOrCreate()\n","\n","# Read from SQLite\n","df_sqlite = pd.read_sql_query(\"SELECT * FROM employee\", conn)\n","df_spark = spark.createDataFrame(df_sqlite)\n","\n","# Show DataFrame (simulates HDFS storage)\n","df_spark.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DCDyrlqvKbP","executionInfo":{"status":"ok","timestamp":1760345551473,"user_tz":-330,"elapsed":24129,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"91b60300-06c3-4452-d209-e34cca2c7ef5"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n","+---+-------+-------+------+\n","| id|   name|   dept|salary|\n","+---+-------+-------+------+\n","|  1|  Alice|     HR| 35000|\n","|  2|    Bob|     IT| 40000|\n","|  3|Charlie|Finance| 38000|\n","+---+-------+-------+------+\n","\n"]}]},{"cell_type":"code","source":["# Register DataFrame as a Spark SQL table (simulates Hive table)\n","df_spark.createOrReplaceTempView(\"employee_hive\")\n","\n","# Query Hive table\n","spark.sql(\"SELECT * FROM employee_hive\").show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xne7912kvOXK","executionInfo":{"status":"ok","timestamp":1760345552312,"user_tz":-330,"elapsed":837,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"22617953-d18e-4b60-f358-2280a9161044"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------+-------+------+\n","| id|   name|   dept|salary|\n","+---+-------+-------+------+\n","|  1|  Alice|     HR| 35000|\n","|  2|    Bob|     IT| 40000|\n","|  3|Charlie|Finance| 38000|\n","+---+-------+-------+------+\n","\n"]}]},{"cell_type":"code","source":["# Export DataFrame back to SQLite (simulates sqoop export)\n","df_export = spark.sql(\"SELECT * FROM employee_hive\")\n","df_export.toPandas().to_sql(\"employee_backup\", conn, if_exists='replace', index=False)\n","\n","# Verify\n","pd.read_sql_query(\"SELECT * FROM employee_backup\", conn)\n"],"metadata":{"id":"T-hFOmZwxT0q","executionInfo":{"status":"ok","timestamp":1760345557120,"user_tz":-330,"elapsed":494,"user":{"displayName":"Tamilarasan D","userId":"02307332032359826995"}},"outputId":"6d5de57e-2f27-4363-a56e-da2525d5c7f1","colab":{"base_uri":"https://localhost:8080/","height":143}},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id     name     dept  salary\n","0   1    Alice       HR   35000\n","1   2      Bob       IT   40000\n","2   3  Charlie  Finance   38000"],"text/html":["\n","  <div id=\"df-07c5bafc-0801-4430-bab7-34358c27a948\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>name</th>\n","      <th>dept</th>\n","      <th>salary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Alice</td>\n","      <td>HR</td>\n","      <td>35000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Bob</td>\n","      <td>IT</td>\n","      <td>40000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Charlie</td>\n","      <td>Finance</td>\n","      <td>38000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07c5bafc-0801-4430-bab7-34358c27a948')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-07c5bafc-0801-4430-bab7-34358c27a948 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-07c5bafc-0801-4430-bab7-34358c27a948');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-4eb02a37-1e27-4822-961c-92f6f495e158\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4eb02a37-1e27-4822-961c-92f6f495e158')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-4eb02a37-1e27-4822-961c-92f6f495e158 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"pd\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Alice\",\n          \"Bob\",\n          \"Charlie\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dept\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"HR\",\n          \"IT\",\n          \"Finance\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"salary\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2516,\n        \"min\": 35000,\n        \"max\": 40000,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          35000,\n          40000,\n          38000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":[],"metadata":{"id":"b_M_AwvdxYYH"},"execution_count":null,"outputs":[]}]}